#The result is multiplied by 2.178657 instead of 3.411878
max(UnaMax$Maxa)/min(UnaMax$Maxa)
#when we multiply a by 2.5, the max doesn't change
max(UnaMax$a)/min(UnaMax$a)
#so the growth by multiplication is of 0.8714626 instead of 1.364751
(max(UnaMax$Maxa)/min(UnaMax$Maxa))/((max(UnaMax$a)/min(UnaMax$a)))
#With a gradient:
((max(UnaMax$Maxa)-min(UnaMax$Maxa)))/((max(UnaMax$a)-min(UnaMax$a)))
#we get 1.276956 instead of 7.235635
#3. Construct a parameter sensitivity indicator and explain it ;
#We choose a single parameter to evaluate
#We iterate over multiple values of a, b and c a chosen number of times
#on a chosen domain for each parameter, and we take the maximum value
#of the model for those parameters.
#We also try to do the same for the same parameters, except that for
#the parameter we evaluate, we increment it a little, and we compare
#both results with a subtraction.
#We then divide the result of that subtraction by the increment of
#the parameter.
#Last we sum all those differences (in absolute value in order to get
#distances), and we divide it by the number of iterations over that parameter
#in order to get an average value.
#The value we get is considered as the sensitivity indicator
avgSensitivyIndicator =
function(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname){
aseq=seq(from=amin,to=amax,length.out=abclen)
bseq=seq(from=bmin,to=bmax,length.out=abclen)
cseq=seq(from=cmin,to=cmax,length.out=abclen)
sensi=0
if (paramname=="a"){
for (i in 1:(abclen-1)){
for (bi in bseq){
for (ci in cseq){
ai1=aseq[i]
ai2=aseq[i+1]
sensi=sensi+
abs(max(Give_Output(U0,abclen,ai2,bi,ci)$Un[[1]])-
max(Give_Output(U0,abclen,ai1,bi,ci)$Un[[1]]))/
abs(ai2-ai1)
}
}
}
}
if (paramname=="b"){
for (i in 1:(abclen-1)){
for (ci in cseq){
for (ai in aseq){
bi1=bseq[i]
bi2=bseq[i+1]
sensi=sensi+
abs(max(Give_Output(U0,abclen,ai,bi2,ci)$Un[[1]])-
max(Give_Output(U0,abclen,ai,bi1,ci)$Un[[1]]))/
abs(bi2-bi1)
}
}
}
}
if (paramname=="c"){
for (i in 1:(abclen-1)){
for (ai in aseq){
for (bi in bseq){
ci1=cseq[i]
ci2=cseq[i+1]
sensi=sensi+
abs(max(Give_Output(U0,abclen,ai,bi,ci2)$Un[[1]])-
max(Give_Output(U0,abclen,ai,bi,ci1)$Un[[1]]))/
abs(ci2-ci1)
}
}
}
}
return(sensi/abclen)
}
U0=1
amin=2
amax=5
bmin=1
bmax=5
cmin=0.1
cmax=0.9
abclen=10
#4. Compute the value of your indicator associated to each parameter ;
paramname="a"
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname)
#141769.6
paramname="b"
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname)
#7578.562
paramname="c"
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname)
#1325626
#5. Conclude about the problems which may be encountered due to a high
#sensitivity of parameters.
#As soon as you change just a little the value of your parameter, the results
#may vary wildly, making it hard to get a conclusion.
#Exercise 2.3 Distribution of the parameters
#1. List and describe the inputs and the outputs of the model ;
#The input is U0, the starting value of the time serie.
#Between the inputs and outputs there are the parameters:
#a
#b
#c
#and N.
#The output is a time series Un.
#2. What can be the law of probability of the parameters a, b and c ?
#Justify your answer
#In the nature, by default, a Normal law can be used.
#3. What can be the law of probability of the inputs ? Justify your answer ;
#Same answer
# 4. Set the parameters of those probability laws at values that look suitable
#to you.
# Specify the chosen values, justify them and present the obtained histograms ;
U0=1
amin=2
amax=5
bmin=1
bmax=5
cmin=0.1
cmax=0.9
abclen=10
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname="a")
#141769.6
amin=0.1
amax=2
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname="a")
#1324.824
amin=0.1
amax=1
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname="a")
#309.6653
amin=0.2
amax=0.5
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname="a")
#222.1958
#a=[0.1;1] appears to be reasonable to minimize the sensitivity,
#while leaving some room to vary the results.
#Besides, reducing the size of the sample doesn't reduce linearly the sensitivity.
amin=0.1
amax=1
bmin=1
bmax=5
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname="b")
#114.1527
bmin=1
bmax=10
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname="b")
#110.5385
bmin=1
bmax=50
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname="b")
#103.7835
bmin=1
bmax=3
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname="b")
#116.997
#The sensibility of the b indicator barely changes, and even gets smaller for
#higher values, meaning its growth must be close to logarithmic given that
#the subtraction of the sensitivity function shrinks it.
#Let us stay with the initial values then: b=[1;5]
bmin=1
bmax=5
cmin=0.1
cmax=0.9
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname="c")
#222.1015
#We can already note that dropping the sensitivity of a by choosing another
#domain dropped the sensitivity
cmin=0.1
cmax=0.5
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname="c")
#85.45952
cmin=0.1
cmax=1
avgSensitivyIndicator(U0,amin,amax,bmin,bmax,cmin,cmax,abclen,paramname="c")
#437.2899
#We can keep the initial domain to have a similar value for the sensitivity
#as we have with parameter a. The sensitivity for the parameter b is about
#half of that value for the chosen domains, and it makes sense to keep its
#sensitivity lower than the one of the other two since it was so much less
#impactful initially.
#So we remain with c=[0.1;0.9]
cmin=0.1
cmax=0.9
#For general of normal laws, it would give at 95% CI:
#a=[0.1;1] => Mean = 0.55, SD = 0.45
#b=[1;5] => Mean = 3, SD = 2
#c=[0.1;0.9] => Mean = 0.5, SD = 0.4
a=rnorm(1,mean=0.55,sd=0.45)
b=rnorm(1,mean=3,sd=2)
c=rnorm(1,mean=0.5,sd=0.4)
a;b;c
#To further reduce the sensitivity, let us halve the standard deviation
par(mfrow=c(1,3))
a_values=rnorm(N,mean=0.55,sd=0.225)
b_values=rnorm(N,mean=3,sd=1)
c_values=rnorm(N,mean=0.5,sd=0.2)
hist(a_values,main="Values of param a")
hist(b_values,main="Values of param b")
hist(c_values,main="Values of param c")
#A few values should still be out of bound, so we can eventually use them to try
#out different results later.
#3 Simulation tests
#Exercise 3.1 The notion of noisy data
#1. Describe the notion of noise in the context of data collection ;
#The noise is the impact of elements that would be external to the model,
#such as tools not working, the weather changes that may be underestimated
#in a period, human errors... It appears to make the data more chaotic.
#2. What problems may be encountered due to the presence of noise ?
#It can be harder to determine the actual law of the model, or the data
#might become unusable.
#3. What can be the law of probability of this random component ?
#A normal law can be used for noise modeling.
#Exercise 3.2 Generation of a learning database
#1. Verify that the parameters of the probability laws of a, b and c previously set,
#ensure the relative stability of the outputs. If it is not the case, modify those
#probability laws in order to ensure it. Fix N at 40 and M at 100 ;
#The stability was already ensured in the code just above.
N=40
M=100
#2. Generate M values of the parameters a, b and c, and M values of U0 ;
a_values=rnorm(M,mean=0.55,sd=0.225)
b_values=rnorm(M,mean=3,sd=1)
c_values=rnorm(M,mean=0.5,sd=0.2)
U0_values=abs(rnorm(M,mean=1,sd=1)) #U0 must be positive, otherwise we get NaN
#3. From those values, generate M curves and build a M x N database. Name it Out-
#put_Curves ;
#Get the list of results corresponding to each M of the combinations in a
#dataframe
dfUn=data.frame(matrix(ncol = ncol(Computed_Outputs), nrow = 0))
names(dfUn)=names(Computed_Outputs)
for (i in 1:M){
dfUn=rbind(dfUn,Give_Output(U0_values[i],N,a_values[i],b_values[i],c_values[i]))
}
#Put all the M curves in a list
List_Curves=list()
library(ggplot2)
par(mfrow=c(1,1))
for (i in 1:M){
unData=as.data.frame(matrix(data=dfUn[i,]$Un[[1]],ncol=1))
colnames(unData)=c("Un")
unData$n=c(1:nrow(unData))
List_Curves[[i]]=
list(N=dfUn[i,]$N,
a=dfUn[i,]$a,
b=dfUn[i,]$b,
c=dfUn[i,]$c,
U0=dfUn[i,]$U0,
Un=dfUn[i,]$Un[[1]],
plot=ggplot(unData, aes(x=c(1:nrow(unData)), y=Un)) + geom_point() +
theme_bw() +
ggtitle(paste("a=",round(x=dfUn[i,]$a,digits =3),
", b=",round(x=dfUn[i,]$b,digits =3),
", c=",round(x=dfUn[i,]$c,digits =3),
", U0=",round(x=dfUn[i,]$U0,digits =3),
", i=", i,sep="")))
}
List_Curves[[2]]
List_Curves[[9]]$plot
#Put all the values in a dataframe
Output_Curves=data.frame(matrix(rep(NA,M*(N+4)),nrow=M,ncol=(N+4)))
dim(Output_Curves)
for (i in 1:M){
dfUni=dfUn[i,]
Output_Curves[i,]=c(dfUni$a,dfUni$b,dfUni$c,dfUni[[1]][[1]])
}
colnames(Output_Curves)=c("a","b","c",paste("U",seq(0,40),sep = ""))
View(Output_Curves)
dim(Output_Curves)
Output_Curves_U=subset(Output_Curves,select = -c(a,b,c))
dim(Output_Curves_U)
Output_Curves_params=subset(Output_Curves,select = c(a,b,c))
dim(Output_Curves_params)
#4. Generate a matrix M x N of random noise components ;
noiseMatrix=as.data.frame(t(rnorm(N+1,mean=0,sd=0.1)))
for (i in 2:M){
noiseMatrix[i,]=t(rnorm(N+1,mean=0,sd=0.1))
}
View(noiseMatrix)
dim(noiseMatrix)
#5. Add those matrices to the generated database Output_Curves. Name it Main
#Learning Base ;
Main_Learning_Base=Output_Curves_U+noiseMatrix
View(Main_Learning_Base)
dim(Main_Learning_Base)
Main_Learning_Base=cbind(Output_Curves_params,Main_Learning_Base)
dim(Main_Learning_Base)
colnames(Main_Learning_Base)=colnames(Output_Curves)
View(Main_Learning_Base)
#6. Build a SQL database, while ensuring to keep all the information used to
#generate each curve ;
#TODO
# 7. Divide the database Main Learning Base into two datasets : a Training
# Dataset (Main Training Base), made of 0:7M curves and a Test Dataset (Main
# Test Base), made of 0:3M curves ;
Main_Training_Base = Main_Learning_Base[1:70, ]
dim(Main_Training_Base)
Main_Test_Base = Main_Learning_Base[71:100, ]
dim(Main_Test_Base)
#Let us take the sum of the norm 2 distance between each point at a same index
#for the difference between two vectors representing the Y-values of the curves
#each having the same length
CurveDiff=function(Curve1,Curve2){
if (length(Curve1)!=length(Curve2)){
return (NaN)
}
sumDiff=sqrt(sum((2*(Curve1-Curve2)/(Curve1+Curve2))**2))/length(Curve1)
sumDiff
}
Curve1=Main_Training_Base[1,4:44]
Curve2=Main_Training_Base[2,4:44]
CurveDiff(Curve1,Curve2)
Curve1=Main_Training_Base[2,4:44]
Curve2=Main_Training_Base[1,4:44]
CurveDiff(Curve1,Curve2)
Curve1=Main_Training_Base[1,4:44]
Curve2=Main_Training_Base[5,4:44]
CurveDiff(Curve1,Curve2)
Curve1=Main_Training_Base[5,4:44]
Curve2=Main_Training_Base[2,4:44]
CurveDiff(Curve1,Curve2)
#T has the role of N, we remove it
#We take the first U0 we find for the corresponding parameters
Rel_Diff=function(a_t , b_t , c_t, TrainingBase=Main_Training_Base){
TrainCurve=TrainingBase[TrainingBase$a==a_t &
TrainingBase$b==b_t &
TrainingBase$c==c_t,][
4:length(TrainingBase)]#Removes a,b,c
#We take the first U0 we find for the corresponding parameters to generate
#a new curve
TestCurve=Generate_Output(TrainCurve$U0[1],N,a_t,b_t,c_t)$Un[[1]]
CurveDiff(TrainCurve,TestCurve)
}
a_t=Main_Training_Base$a[1]
b_t=Main_Training_Base$b[1]
c_t=Main_Training_Base$c[1]
Rel_Diff(a_t , b_t , c_t)
# 4. Let All_U0 a vector containing a list of values of U0. Describe what the
# following function does :
a_t=0.5
b_t=3
c_t=0.6
All_U0=seq(from=1, to=10, by=1)
Predicted_Curves <- mapply(Generate_Output, All_U0 , N, a_t , b_t , c_t )
Predicted_Curves
f_Predicted_Curves=function(All_U0 , N, a_t , b_t , c_t){
mapply(Generate_Output, All_U0 , N, a_t , b_t , c_t )
}
f_obj = function(a_t , b_t , c_t, TrainingBase=Main_Training_Base){
#Takes the values of the time series without a,b,c
TrainCurve=TrainingBase[TrainingBase$a==a_t &
TrainingBase$b==b_t &
TrainingBase$c==c_t,][4:length(TrainingBase)]
#We try all the curves for each U0 in the Training Base
TestCurves=f_Predicted_Curves(TrainCurve$U0 , N, a_t , b_t , c_t)
tmpDiff=c()
for (i in 1:ncol(TestCurves)){
tmpDiff=c(tmpDiff,CurveDiff(TrainCurve[i,],TestCurves[,i]$Un[[1]]))
}
mean(tmpDiff)
}
a_t=Main_Training_Base$a[2]
b_t=Main_Training_Base$b[2]
c_t=Main_Training_Base$c[2]
f2_test=f_obj(a_t , b_t , c_t);f2_test
rd2_test=Rel_Diff(a_t , b_t , c_t);rd2_test
rd2_test==f2_test
a_t=Main_Training_Base$a[1]
b_t=Main_Training_Base$b[1]
c_t=Main_Training_Base$c[1]
f1_test=f_obj(a_t , b_t , c_t);f1_test
rd1_test=Rel_Diff(a_t , b_t , c_t);rd1_test
rd1_test==f1_test
#f_obj and Rel_Diff return the same results for identical parameters.
#This function seems to work, but we do not have at least two lines with the
#same combination a, b and c (and in addition a different U0) in the database.
#So let us add a temporary row that would suit us.
Copy_Training_Base=Main_Training_Base
rowToClone=Copy_Training_Base[1,]
#create a copy of the row with another U0
tempUn=Generate_Output(rowToClone$U0+1, N, rowToClone$a, rowToClone$b, rowToClone$c)
#We confirm that we use the same parameters as in the above test
c(a_t , b_t , c_t)==c(rowToClone$a, rowToClone$b, rowToClone$c)
#Add some noise
tempUn$Un[[1]]=tempUn$Un[[1]]+t(rnorm(N+1,mean=0,sd=0.1))
rowToAdd=c(tempUn$a,tempUn$b,tempUn$c,  tempUn$Un[[1]])
Copy_Training_Base[nrow(Copy_Training_Base) + 1,] = rowToAdd
f1b_test=f_obj(rowToClone$a , rowToClone$b , rowToClone$c, Copy_Training_Base)
f1b_test
#There are now 2 rows that are taken into account for f_obj, and it still works
c(f1_test,f1b_test) #As expected, the results are slightly different
library(nloptr)
#a=[0.1;1] => Mean = 0.55, SD = 0.45
#b=[1;5] => Mean = 3, SD = 2
#c=[0.1;0.9] => Mean = 0.5, SD = 0.4
?direct
?sample
baseSplit = sample(c(TRUE, FALSE), 50, replace=TRUE)
baseSplit
baseSplit = sample(c(TRUE, FALSE), nrow(Main_Training_Base), replace=TRUE)
baseSplit
New_Training_Base=Main_Training_Base[baseSplit,]
dim(New_Training_Base)
New_Training_Base = sample(x=Main_Training_Base, size=50, replace=TRUE)
dim(New_Training_Base)
New_Training_Base = sample.int(x=Main_Training_Base, size=50, replace=TRUE)
New_Training_Base = sample(x=Main_Training_Base, size=50, replace=TRUE)
dim(New_Training_Base)
View(New_Training_Base)
New_Training_Base=Main_Training_Base[sample(nrow(Main_Training_Base), 50), ]
#New_Training_Base = sample(x=Main_Training_Base, size=50, replace=TRUE)
dim(New_Training_Base)
New_Training_Base=Main_Training_Base[sample(nrow(Main_Training_Base), 50, replace=TRUE), ]
#New_Training_Base = sample(x=Main_Training_Base, size=50, replace=TRUE)
dim(New_Training_Base)
#TODO
#Doesn't work?
f_obj(0.1,1,0.1)
f_obj(min(Main_Training_Base$a),min(Main_Training_Base$b),
min(Main_Training_Base$c))
# 2. By using the function f obj and the algorithm DIRECT, adjust the parameters
# a, b and c from those new training database. What are the obtained values of
# the parameters a, b and c ?
direct(f_obj,lower=c(min(New_Training_Base$a),min(New_Training_Base$b),
min(New_Training_Base$c)),
upper=c(max(New_Training_Base$a),max(New_Training_Base$b),
max(New_Training_Base$c)))
# 4. Calculate the average, the relative variance and the relative standard
# deviation associated to each parameter ;
mean(at_10)
# 3. Repeat this operation (Question 1 then 2) 10 times and build a table
# containing the values of the 10 obtained triplets (at; bt; ct) ;
at_10=c()
bt_10=c()
ct_10=c()
# 4. Calculate the average, the relative variance and the relative standard
# deviation associated to each parameter ;
mean(at_10)
at_10=rep(0,10)
bt_10=rep(0,10)
ct_10=rep(0,10)
# 4. Calculate the average, the relative variance and the relative standard
# deviation associated to each parameter ;
mean(at_10)
var(at_10)
sd(at_10)
# 2. By using the chosen indicators, compute the accuracy of the model on the
# Training Dataset ;
chisq.test(Main_Training_Base$a)
chisq.test(Main_Training_Base$b)
chisq.test(Main_Training_Base$c)
# 2. By using the chosen indicators, compute the accuracy of the model on the
# Training Dataset ;
chisq.test(Main_Training_Base$a, rescale.p=TRUE)
chisq.test(Main_Training_Base$b, rescale.p=TRUE)
chisq.test(Main_Training_Base$c, rescale.p=TRUE)
chisq.test(Main_Test_Base$a, rescale.p=TRUE)
chisq.test(Main_Test_Base$b, rescale.p=TRUE)
chisq.test(Main_Test_Base$c, rescale.p=TRUE)
#Kolmogorov–Smirnov test
num_of_samples = 100000
y <- rgamma(num_of_samples, shape = 10, scale = 3)
resa <- CramerVonMisesTwoSamples(Main_Training_Base$a,y)
install.packages("CDFt")
#Kolmogorov–Smirnov test
#install.packages("CDFt")
library(CDFt)
resa <- CramerVonMisesTwoSamples(Main_Training_Base$a,y)
p-valuea = 1/6*exp(-resa)
resa <- CramerVonMisesTwoSamples(Main_Training_Base$a,y)
p_valuea = 1/6*exp(-resa);p_valuea
resb <- CramerVonMisesTwoSamples(Main_Training_Base$b,y)
p_valueb = 1/6*exp(-resb);p_valueb
resc <- CramerVonMisesTwoSamples(Main_Training_Base$c,y)
p_valuec = 1/6*exp(-resc);p_valuec
resc
resulta = ks.test(Main_Training_Base$a, y);resulta
resultb = ks.test(Main_Training_Base$b, y);resultb
resultc = ks.test(Main_Training_Base$c, y);resultc
# 3. As previously, compute the accuracy of the model on the Test Dataset ;
#Chi Square test
chisq.test(Main_Test_Base$a, rescale.p=TRUE)
chisq.test(Main_Test_Base$b, rescale.p=TRUE)
chisq.test(Main_Test_Base$c, rescale.p=TRUE)
#Kolmogorov–Smirnov test
#install.packages("CDFt")
library(CDFt)
num_of_samples = 100000
y <- rgamma(num_of_samples, shape = 10, scale = 3)
resa <- CramerVonMisesTwoSamples(Main_Test_Base$a,y)
p_valuea = 1/6*exp(-resa);p_valuea
resb <- CramerVonMisesTwoSamples(Main_Test_Base$b,y)
p_valueb = 1/6*exp(-resb);p_valueb
resc <- CramerVonMisesTwoSamples(Main_Test_Base$c,y)
p_valuec = 1/6*exp(-resc);p_valuec
#Cramér–von Mises criterion
num_of_samples = 100000
y <- rgamma(num_of_samples, shape = 10, scale = 3)
resulta = ks.test(Main_Test_Base$a, y);resulta
resultb = ks.test(Main_Test_Base$b, y);resultb
resultc = ks.test(Main_Test_Base$c, y);resultc
# https://www.rdocumentation.org/packages/rootSolve/versions/1.8.2.2/topics/gradient
install.packages("rootSolve")
# https://www.rdocumentation.org/packages/rootSolve/versions/1.8.2.2/topics/gradient
#install.packages("rootSolve")
library(rootSolve)
?gradient
install.packages("gradDescent")
# http://eric.univ-lyon2.fr/~ricco/tanagra/fichiers/fr_Tanagra_Gradient_Descent_R.pdf
#install.packages("gradDescent")
library(gradDescent)
?gradDescentR
?gradDescentR.learn
